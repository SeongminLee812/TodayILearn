# 신경세포의 모델화
인공신경망(ANN, Artificial Neural Network)
1. 각 입력에는 가중치를 곱함
2. 입력과 가중치의 곱을 모두 합한 값에 편향이라는 상수를 더함
3. 활성화 함수에 의해 처리됨

# 뉴런의 네트워크 화
신경망은 입력층, 은닉층(중간층), 출력층 3개의 층으로 분류됨
순전파 : 신경망에서 입력으로부터 시작해 출력으로 정보가 전달되는 것
역전파 : 출력에서 입력방향으로 정보가 반대방향으로 전달되는 것
- 이전 층의 뉴런 수가 m, 다음 층의 뉴런 수가 n이라면 두 층 사이에는 m x n개의 가중치가 있음 -> m x n 행렬로 모든 가중치를 표시할 수 있다.
- 앞층의 출력과 뒤층의 입력은 벡터로 표시
- 편향 :  벡터로 표시
- 앞층의 입력 벡터와 가중치 행렬을 행렬곱으로 연산 -> 뒤층 도 연산 후 편향을 더해 활성화 함수로 출력 (벡터의 각 원소값을 활성화 함수로 처리)

# 활성화 함수
1. 계단 함수 : 함수의 입력값이 0이하이면 출력값은 0, 0보다 크면 출력값은 1
	- 0이나 1로 간단하게 표시할 수 있으나, 0과 1의 중간상태를 나타낼 수 없음
	- ![[Pasted image 20221101154541.png]]
2. 시그모이드 함수
	- $$\begin{align*}
y=\frac{1}{1+exp(-x)}
\end{align*}$$
	- ![[Pasted image 20221101154808.png]]
	- 함수의 입력값이 작을 수록 출력값은 0에 가깝고 입력값이 커지면 출력값은 1에 근접
3. 하이퍼볼릭 탄젠트 함수
	- $$\begin{align*}
y=\frac{exp(x)-exp(-x)}{exp(x)+exp(-x)}
\end{align*}$$
	- ![[Pasted image 20221101154940.png]]
	- -1에서 1사이에서 매끄러운 곡선으로 변화하는 함수 0을 중심으로 대칭이기 때문에 밸런스가 좋은 함수

4. ReLU
	- ReLU는 램프함수라고도 불리며, x>0 범위에서 우상향으로 뻗어 올라가는 모양
	- 간단하면서도, 층의 수가 많아져도 안정적으로 학습할 수 있다. 최근 딥러닝에서 가장 많이 사용
	- $y=0(x \le 0)  \,\,\,\,\, y=1(x>0)$
	- ![[Pasted image 20221101155316.png]]

5. Leakly ReLU
	- ReLU를 개선한 것으로, ReLU 그래프와 비슷한 듯 보이지만 자세히 보면 x가 음수인 영역에서 그래프 직선이 조금 기울어져 있음
	- $y=0.01x(x \le 0) \,\,\,\,\,\,\, y=x(x>0)$
	- $x \le 0$일 때 앞에 작은 계수가 붙어있는 특징
	- x가 음수인 영역에서 dying ReLU(출력이 0이되어 학습이 진행되지 않는 뉴런이 발생하는 현상)를 피할 수 있음

6. 항등함수
	- ![[Pasted image 20221101155803.png]]

7. 소프트 맥수 함수
	- 분류 문제를 다룰 때 적합한 활성화 함수
	- 출력된 모든 원솟값이 0에서 1사이의 범위에 있으며 합계는 1임
- 