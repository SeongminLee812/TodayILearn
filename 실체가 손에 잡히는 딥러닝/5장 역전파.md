# 학습규칙
1. 헵의 규칙
2. 델타규칙 : 
	- 출력값과 정답의 오차가 커질수록 가중치 수정량도 커진다
	- 입력값이 커질수록 가중치 수정량이 커진다

# 역전파
- 신경망을 학습시킬 때 이용하는 알고리즘, 출력값과 정답의 오차를 네트워크에서 역전파시켜 네트워크의 가중치와 편향을 최적화
- 먼저 순전파로 얻은 출력값과 정답과의 오차를 하나씩 층을 거슬러 올라가며 역방향으로 전파시킴
- 전파시킨 오차에 근거해 각 층의 가중치와 편향의 수정량을 구함
- 이후 모든 층의 가중치와 편향을 조금씩 수정

## Train data Test data
- 원핫 인코딩 -> 범주형 범수를 표현하는 방법으로 값 1개만 True(hot)이고 나머지 값은 모두 False(cold)인 인코딩
	- 1이 하나이고 나머지가 0인 수치벡터를 만드는 과정

## 손실함수
- 출력값과 정답의 오차를 정의하는 함수
- 오차제곱합(Sum of Squares for Error, SSE)
	- 출력층의 모든 뉴련에서 출력값과 정답의 차이를 제곰하고 이 값들들 모두 합한 것
	- yk를 ㅇ각 출력층의 출력값, tk를 정답이라고 할 떄 아래 공식의로 정의됨(미분을 쉽게하기 위해서 1/2을 붙임)
	- $E=\frac{1}{2}\sum_{k} (y_{k}-t_{k})^2$
- 교차 엔트로피 오차(Cross Entropy Error, CEE)
	- 두 분포간의 차이를 나타내는 척도로서 분류 문제에서 많이 사용됨
	- $E=-\sum_{k}tk\log(y_{k})$
	- 크로스 엔트로피의 장점 : 출력값과 정답의 차이가 클 때 학습 속도가 빠르다는 점

## 경사 하강법
- 오차를 차례차례 이전 층으로 전파시켜 가중치와 편향을 조금씩 수정하면서 최적화하기 위한 알고리즘
- 어떤 파라미터 $x_{k}$의 변화량에 대한 함수 $y(x_{1}, x_{2}, ... , x_{k})$의 변화율, 즉 기울기를 구해 이 기울기에 따라 파라미터를 조정하고 $y$를 최적화하는 알고리즘
- 역전파에서는 손실 함수로 구한 오차값을 기점으로 신경망의 반대 방향으로 가중치와 편향을 수정해 나가는데 이때 경사 하강법으로 수정량을 결정 -> 경사 하강법에서는 오차가 줄어들도록 신경망의 가중치와 오차를 조정
- 오차가 최소가 되는 지점을 찾기 위해 기울기를 구하는 것임(편향도)
- ![[Pasted image 20221105213720.png]]
- 국소적인 최소점에 빠져 더이상 가중치 수정을 하지 않는 경우가 있음.
- ![[Pasted image 20221105214007.png]]
- 경사 하강법으로 가중치와 편향을 수정할 때, w를 가중치, b를 편향, E를 오차라고 하면 편미분을 이용해 위와 같은 식으로 나타낼 수 있음
- 위 식에서 w - 에  n