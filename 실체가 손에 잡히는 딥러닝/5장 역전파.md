# 학습규칙
1. 헵의 규칙
2. 델타규칙 : ㅇㅇ
	- 출력값과 정답의 오차가 커질수록 가중치 수정량도 커진다
	- 입력값이 커질수록 가중치 수정량이 커진다

# 역전파
- 신경망을 학습시킬 때 이용하는 알고리즘, 출력값과 정답의 오차를 네트워크에서 역전파시켜 네트워크의 가중치와 편향을 최적화
- 먼저 순전파로 얻은 출력값과 정답과의 오차를 하나씩 층을 거슬러 올라가며 역방향으로 전파시킴
- 전파시킨 오차에 근거해 각 층의 가중치와 편향의 수정량을 구함
- 이후 모든 층의 가중치와 편향을 조금씩 수정

## Train data Test data
- 원핫 인코딩 -> 범주형 범수를 표현하는 방법으로 값 1개만 True(hot)이고 나머지 값은 모두 False(cold)인 인코딩
	- 1이 하나이고 나머지가 0인 수치벡터를 만드는 과정

## 손실함수
- 출력값과 정답의 오차를 정의하는 함수
- 오차제곱합(Sum of Squares for Error, SSE)
	- 출력층의 모든 뉴련에서 출력값과 정답의 차이를 제곱하고 이 값들들 모두 합한 것
	- yk를 각 출력층의 출력값, tk를 정답이라고 할 때 아래 공식으로 정의됨(미분을 쉽게하기 위해서 1/2을 붙임)
	- $E=\frac{1}{2}\sum_{k} (y_{k}-t_{k})^2$
- 교차 엔트로피 오차(Cross Entropy Error, CEE)
	- 두 분포간의 차이를 나타내는 척도로서 분류 문제에서 많이 사용됨
	- $E=-\sum_{k}tk\log(y_{k})$
	- 크로스 엔트로피의 장점 : 출력값과 정답의 차이가 클 때 학습 속도가 빠르다는 점

## 경사 하강법
- 오차를 차례차례 이전 층으로 전파시켜 가중치와 편향을 조금씩 수정하면서 최적화하기 위한 알고리즘
- 어떤 파라미터 $x_{k}$의 변화량에 대한 함수 $y(x_{1}, x_{2}, ... , x_{k})$의 변화율, 즉 기울기를 구해 이 기울기에 따라 파라미터를 조정하고 $y$를 최적화하는 알고리즘
- 역전파에서는 손실 함수로 구한 오차값을 기점으로 신경망의 반대 방향으로 가중치와 편향을 수정해 나가는데 이때 경사 하강법으로 수정량을 결정 -> 경사 하강법에서는 오차가 줄어들도록 신경망의 가중치와 오차를 조정
- 오차가 최소가 되는 지점을 찾기 위해 기울기를 구하는 것임(편향도)
- ![[Pasted image 20221105213720.png]]
- 국소적인 최소점에 빠져 더이상 가중치 수정을 하지 않는 경우가 있음.
- ![[Pasted image 20221105214007.png]]
- 경사 하강법으로 가중치와 편향을 수정할 때, w를 가중치, b를 편향, E를 오차라고 하면 편미분을 이용해 위와 같은 식으로 나타낼 수 있음
- 위 식에서 w - 에  n(에타)는 학습률(learning rate)이라고 부르는 상수이고, 뒤에가 기울기임.
	- 학습률은 학습의 속도를 결정하는 상수. 0.1또는 0.01 등 작은 값이 많이 사용됨.
		- 학습률의 값이 작으면 -> 학습시간 증가, 국소최적해에 빠지는 문제
		- 학습률의 값이 크면 -> 오차가 수렴되기 어려운 문제
		- 효율적인 전역최적해에 도달하기 위해서 학습률을 적절하게 설정할 필요


# 최적화 알고리즘
- 한밤에 산악지대에서 골짜기 아래로 내려가는 전략과 비슷함. 현 시점에서 경사가 급한 방향으로 내려가는 것이 좋을 지, 아니면 그때까지의 경로를 고려해 진행방향을 결정하는 것이 좋을 지 등, 골짜리글 내려가 평지로 도달하기 위해 다양한 전략을 고민해볼 수 있음.
### 1. 확률적 경사 하강법
- Stochastic Gradient Descent(SGD)는 수정할 때마다 샘플을 무작위로 선택하는 알고리즘
- 확률적 경사 하강법은 기울기를 수정할 때마다 훈련 데이터 중에서 무작위로 샘플을 선택하기 때문에 국소 최적해에 잘 빠지지 않는 장점이 있음
- 학습률과 기울기를 곱해 간단하게 수정량이 결정됨
- 간단한 코드로 구현할 수 있음
- 반면, 학습의 진행과정에 따라 수정량을 유연하게 조정할 수 없다는 단점

### 2. 모멘텀
- 모멘텀은 확률적 경사 하강법에 관성을 더한 알고리즘.
- 관성항에 의해 새로운 섲ㅇ량은 그때까지의 수정량으로부터 영향을 받게 되므로, 수정량이 급격하게 변화하는 것을 막고, 조금 더 부드럽게 수정됨.
- 그러나, 설정해야하는 상수가 두개로 늘어나 조정이 조금 더 어려워짐

### 3. 아다그라드(AdaGrad, Adaptive Gradient)
- 학습이 진행되면서 알고리즘에 의해 학습률이 조금씩 감소하게됨.
- 처음에는 넓은 영역에서 탐색을 시작해 점차 탐색 범위를 좁혀가는 효율적인 탐색이 가능해짐
- 수정량이 감소하기 때문에 도중에 수정량이 0이 되어버려 더는 최적화가 진행되지 않을 수 있다.

### 4. RMSprop
- 아다그라드에서 수정량이 감소해 학습이 정체되는 단점을 극복한 것

### 5. 아담(Adam, ADAptive Moment estimation)
- 크게 보면 모멘텀 알고리즘과 아다그라드 알고리즘을 통합한 것으로 볼 수 있는데 아담의 로직을 제대로 설명하기 어려움


# 배치사이즈
### 에포크와 배치
- 모든 훈련 데이터를 1회 학습하는 것을 1에포크라고 한다.
- 1에포크에 모든 학습 데이터를 사용하게 되고, 훈련 데이터의 샘플은 여러개로 묶여 학습에 이용되는데, 이 샘플의 그룹을 배치라고 한다.
- 배치 사이즈 : 하나의 배치에 포함되는 샘플의 수.


# 행렬연산
- 행렬
