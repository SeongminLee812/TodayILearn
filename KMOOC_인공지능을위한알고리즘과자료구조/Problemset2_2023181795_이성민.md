### Problem 1 Solution

To find the optimal vector $w^*$ that separates two classes in a dataset $D = \{(x_i, y_i)\}_{i=1}^N$, we maximize the following objective function:

$$
w^* = \arg\max_w \frac{w^\top \Sigma w}{w^\top S_W w}
$$

where matrices $\Sigma$ and $S_W$ are defined as:

$$
\Sigma = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)(x_i - \mu)^\top
$$

$$
S_W = \frac{1}{N} \left( \sum_{i: y_i=1} (x_i - \mu_1)(x_i - \mu_1)^\top + \sum_{i: y_i=2} (x_i - \mu_2)(x_i - \mu_2)^\top \right)
$$

with mean vectors:

$$
\mu = \frac{1}{N} \sum_{i=1}^N x_i, \quad \mu_1 = \frac{1}{N_1} \sum_{i: y_i=1} x_i, \quad \mu_2 = \frac{1}{N_2} \sum_{i: y_i=2} x_i
$$

Assuming $S_W$ is invertible, the closed-form solution for $w^*$ is:

$$
w^* = \alpha S_W^{-1} (\mu_1 - \mu_2)
$$

where $\alpha \in \mathbb{R}$ is a constant.

---
### Problem 2 Solution

For a dataset $D = \{x_i\}_{i=1}^N$ with $x_i \in \mathbb{R}^D$ and the mean $\mu = \frac{1}{N} \sum_{i=1}^N x_i$, we aim to minimize the residual error in principal component analysis (PCA). The residual error for PCA on the dataset $D$ is defined as:

$$
w^*, \{a_i^*\}_{i=1}^N = \arg\min_{w, \{a_i\}_{i=1}^N} \sum_{i=1}^N \| x_i - (\mu + a_i w) \|^2
$$

where $w \in \mathbb{R}^D$ and $a_i \in \mathbb{R}$ for $i = 1, \dots, N$.

#### (1) Finding $a_i$ to Minimize Residual Error

To find the optimal $a_i$ that minimizes the residual error as a function of $x_i$, $\mu$, and $w$, we take the partial derivative with respect to $a_i$ and set it to zero:

$$
\frac{\partial}{\partial a_i} \| x_i - (\mu + a_i w) \|^2 = 0
$$

This allows us to express $a_i$ in terms of $x_i$, $\mu$, and $w$.

#### (2) Showing $w$ is an Eigenvector of $\Sigma$

Given that $\Sigma = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)(x_i - \mu)^\top$, we aim to show that $w$, which minimizes the residual error, is the eigenvector of $\Sigma$ corresponding to the largest eigenvalue.

This objective aligns with PCA, where $w$ is the principal component direction that maximizes the variance of the data. Hence, $w$ is the eigenvector of $\Sigma$ associated with its largest eigenvalue.

---
### Problem 3 Solution

In this task, we analyze the Breast Cancer dataset using logistic regression to identify the most important features for classification.

#### Steps:

1. **Data Loading and Exploration**
   - Loaded the Breast Cancer dataset from `scikit-learn`.
   - Visualized the distribution of each feature across both classes.

2. **Data Preprocessing**
   - Performed feature scaling (standardization).
   - Split the dataset into training (70%) and testing (30%) sets.

3. **Model Training**
   - Trained a logistic regression model on the training set.
   - Evaluated the modelâ€™s performance on the test set.

4. **Feature Importance Analysis**
   - Extracted the absolute values of the coefficients from the trained logistic regression model.
   - Sorted the coefficients to analyze feature importance.

#### Visualization of Feature Importances

The following bar plot shows the importance of each feature based on the magnitude of logistic regression coefficients:
![[feature_importance_breast_cancer 1.png]]

The features with the largest absolute coefficient values are considered the most important for classification. This plot provides a clear view of which features contribute most significantly to the model's predictions.

---
