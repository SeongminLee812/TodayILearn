### Problem 1 Solution

To find the optimal vector $w^*$ that separates two classes in a dataset $D = \{(x_i, y_i)\}_{i=1}^N$, we maximize the following objective function:

$$
w^* = \arg\max_w \frac{w^\top \Sigma w}{w^\top S_W w}
$$

where matrices $\Sigma$ and $S_W$ are defined as:

$$
\Sigma = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)(x_i - \mu)^\top
$$

$$
S_W = \frac{1}{N} \left( \sum_{i: y_i=1} (x_i - \mu_1)(x_i - \mu_1)^\top + \sum_{i: y_i=2} (x_i - \mu_2)(x_i - \mu_2)^\top \right)
$$

with mean vectors:

$$
\mu = \frac{1}{N} \sum_{i=1}^N x_i, \quad \mu_1 = \frac{1}{N_1} \sum_{i: y_i=1} x_i, \quad \mu_2 = \frac{1}{N_2} \sum_{i: y_i=2} x_i
$$

Assuming $S_W$ is invertible, the closed-form solution for $w^*$ is:

$$
w^* = \alpha S_W^{-1} (\mu_1 - \mu_2)
$$

where $\alpha \in \mathbb{R}$ is a constant.

---
### Problem 2 Solution

For a dataset $D = \{x_i\}_{i=1}^N$ with $x_i \in \mathbb{R}^D$ and the mean $\mu = \frac{1}{N} \sum_{i=1}^N x_i$, we aim to minimize the residual error in principal component analysis (PCA). The residual error for PCA on the dataset $D$ is defined as:

$$
w^*, \{a_i^*\}_{i=1}^N = \arg\min_{w, \{a_i\}_{i=1}^N} \sum_{i=1}^N \| x_i - (\mu + a_i w) \|^2
$$

where $w \in \mathbb{R}^D$ and $a_i \in \mathbb{R}$ for $i = 1, \dots, N$.

#### (1) Finding $a_i$ to Minimize Residual Error

To find the optimal $a_i$ that minimizes the residual error as a function of $x_i$, $\mu$, and $w$, we take the partial derivative with respect to $a_i$ and set it to zero:

$$
\frac{\partial}{\partial a_i} \| x_i - (\mu + a_i w) \|^2 = 0
$$

This allows us to express $a_i$ in terms of $x_i$, $\mu$, and $w$.

#### (2) Showing $w$ is an Eigenvector of $\Sigma$

Given that $\Sigma = \frac{1}{N} \sum_{i=1}^N (x_i - \mu)(x_i - \mu)^\top$, we aim to show that $w$, which minimizes the residual error, is the eigenvector of $\Sigma$ corresponding to the largest eigenvalue.

This objective aligns with PCA, where $w$ is the principal component direction that maximizes the variance of the data. Hence, $w$ is the eigenvector of $\Sigma$ associated with its largest eigenvalue.

---
### Problem 3 Solution

In this task, we analyze the Breast Cancer dataset using logistic regression to identify the most important features for classification.

#### Steps:

1. **Data Loading and Exploration**
   - Loaded the Breast Cancer dataset from `scikit-learn`.
   - Visualized the distribution of each feature across both classes.

2. **Data Preprocessing**
   - Performed feature scaling (standardization).
   - Split the dataset into training (70%) and testing (30%) sets.

3. **Model Training**
   - Trained a logistic regression model on the training set.
   - Evaluated the modelâ€™s performance on the test set.

4. **Feature Importance Analysis**
   - Extracted the absolute values of the coefficients from the trained logistic regression model.
   - Sorted the coefficients to analyze feature importance.

#### Visualization of Feature Importances

The following bar plot shows the importance of each feature based on the magnitude of logistic regression coefficients:
![[feature_importance_breast_cancer 1.png]]

The features with the largest absolute coefficient values are considered the most important for classification. This plot provides a clear view of which features contribute most significantly to the model's predictions.

---
### Problem 4 Solution

For classes '1', '5', and '6' of the MNIST dataset, we apply PCA and FDA for dimensionality reduction and visualization.

#### (1) PCA Projection

We apply PCA on the training data and project it onto the first two principal components. The plot shows class separation for the training and test sets.

![[pca_projection_mnist.png]]
#### (2) PCA Reconstruction for Different Dimensions

Using $k \in \{1, 2, 5, 10, 20, 50, 100\}$ components, we reconstruct the training images. As $k$ increases, the quality of reconstruction improves.

#### (3) FDA Projection

We apply FDA on the training data and project it onto the first two discriminant directions. The plot shows class separation for training and test sets.

![[fda_projection_mnist.png]]

#### (4) 1-NN Classification on PCA and FDA Representations

Using 1-NN on PCA-reduced data with different dimensions:
- PCA accuracy increases with $k$ but may overfit at high dimensions.

For FDA-reduced data, 1-NN accuracy generally outperforms PCA at lower dimensions due to optimal class separation.


---

### Problem 5 Solution

This analysis involves sampling functions from Gaussian processes (GP) with different covariance functions and performing regression with a composite kernel.

#### (1) Covariance Functions and Random Samples

We generate random samples from GP with the following covariance functions:
- **Constant kernel**: $k(x, x') = \theta_0^2$
- **Linear kernel**: $k(x, x') = \theta_0^2 + \theta_1^2 x^\top x'$
- **RBF kernel**: $k(x, x') = \theta_0^2 + \theta_1^2 \exp\left(-\frac{\|x - x'\|^2}{\theta_2^2}\right)$

#### Visualizations of GP Samples

- Constant Kernel:
![[gp_constant_kernel_samples.png]]

- Linear Kernel:![[gp_linear_kernel_samples.png]]


- RBF Kernel:![[gp_rbf_kernel_samples.png]]


#### (2) Gaussian Process Regression with Composite Kernel

Using the composite kernel:
$$
k(x, x') = \theta_0^2 + \theta_1^2 \exp\left(-\frac{\|x - x'\|^2}{\theta_2^2}\right) + \theta_3^2 x^\top x'
$$
we perform GP regression on the provided data points. The prediction and $ \pm 1$ standard deviation confidence interval are shown in the plot below.
![[gp_regression_confidence_interval.png]]